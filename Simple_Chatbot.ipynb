{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_Chatbot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUQyjDw/tfhdg/h2Uj3PMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardiksiloiya/GANs/blob/main/Simple_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AUCX3fn1Iwh"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Activation,LSTM,GRU,Dense,Embedding,Input\n",
        "import glob\n",
        "from keras.models import Model\n",
        "import yaml\n",
        "from keras import preprocessing,utils\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIIXzay_1Qej",
        "outputId": "58bdb1bf-dec5-469c-9190-12187d8d70d9"
      },
      "source": [
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true -O chatbot_nlp.zip\n",
        "!unzip chatbot_nlp.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-23 08:38:22--  https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip [following]\n",
            "--2021-06-23 08:38:22--  https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip [following]\n",
            "--2021-06-23 08:38:22--  https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24292 (24K) [application/zip]\n",
            "Saving to: ‘chatbot_nlp.zip’\n",
            "\n",
            "chatbot_nlp.zip     100%[===================>]  23.72K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-06-23 08:38:23 (11.2 MB/s) - ‘chatbot_nlp.zip’ saved [24292/24292]\n",
            "\n",
            "Archive:  chatbot_nlp.zip\n",
            "   creating: chatbot_nlp/\n",
            "   creating: chatbot_nlp/data/\n",
            "  inflating: chatbot_nlp/data/ai.yml  \n",
            "  inflating: chatbot_nlp/data/botprofile.yml  \n",
            "  inflating: chatbot_nlp/data/computers.yml  \n",
            "  inflating: chatbot_nlp/data/emotion.yml  \n",
            "  inflating: chatbot_nlp/data/food.yml  \n",
            "  inflating: chatbot_nlp/data/gossip.yml  \n",
            "  inflating: chatbot_nlp/data/greetings.yml  \n",
            "  inflating: chatbot_nlp/data/health.yml  \n",
            "  inflating: chatbot_nlp/data/history.yml  \n",
            "  inflating: chatbot_nlp/data/humor.yml  \n",
            "  inflating: chatbot_nlp/data/literature.yml  \n",
            "  inflating: chatbot_nlp/data/money.yml  \n",
            "  inflating: chatbot_nlp/data/movies.yml  \n",
            "  inflating: chatbot_nlp/data/politics.yml  \n",
            "  inflating: chatbot_nlp/data/psychology.yml  \n",
            "  inflating: chatbot_nlp/data/science.yml  \n",
            "  inflating: chatbot_nlp/data/sports.yml  \n",
            "  inflating: chatbot_nlp/data/trivia.yml  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYrpbI8W2ASj"
      },
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for file in glob.glob('chatbot_nlp/data/*'):\n",
        "    stream = open(file,'rb')\n",
        "    info=yaml.safe_load(stream)\n",
        "    conversations=info['conversations']\n",
        "    for con in conversations:\n",
        "        if len(con)>2 :\n",
        "            questions.append(con[0])\n",
        "            replies=con[1:]\n",
        "            ans=''\n",
        "            for rep in replies:\n",
        "                ans+=' ' + rep\n",
        "            answers.append(ans)\n",
        "        elif len(con)> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range(len(answers)):\n",
        "    if type(answers[i]) == str:\n",
        "        answers_with_tags.append(answers[i])\n",
        "    else:\n",
        "        questions.pop(i)\n",
        "\n",
        "answers = list()\n",
        "for i in range(len(answers_with_tags)) :\n",
        "    answers.append('<start> '+answers_with_tags[i]+' <end>')\n",
        "\n",
        "tokenizer=preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions+answers)\n",
        "size=len( tokenizer.word_index )+1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV_QIKiZ-iD1"
      },
      "source": [
        "def tokenize(sentences):\n",
        "    token_list=[]\n",
        "    vocabu=[]\n",
        "    for sentence in sentences:\n",
        "        sentence=sentence.lower()\n",
        "        sentence=re.sub('[^a-zA-Z]',' ',sentence)\n",
        "        tokens=sentence.split()\n",
        "        vocabu+=tokens\n",
        "        token_list.append(tokens)\n",
        "    return token_list,vocabu"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTj3NMrk9ZmP",
        "outputId": "7d60ab45-a14e-42b7-c999-d420edde482e"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "token_questions=tokenizer.texts_to_sequences(questions)\n",
        "m=max([len(x) for x in token_questions])\n",
        "pad_questions=preprocessing.sequence.pad_sequences(token_questions,maxlen=m ,padding='post')\n",
        "encoder_input_data=np.array(pad_questions)\n",
        "print(encoder_input_data.shape,m)\n",
        "\n",
        "token_answers=tokenizer.texts_to_sequences(answers)\n",
        "m2=max([len(x) for x in token_answers])\n",
        "pad_answers=preprocessing.sequence.pad_sequences(token_answers,maxlen=m2,padding='post')\n",
        "decoder_input_data=np.array(pad_answers)\n",
        "print(decoder_input_data.shape,m2)\n",
        "\n",
        "token_answers=tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(token_answers)):\n",
        "    token_answers[i]=token_answers[i][1:]\n",
        "pad_answers=preprocessing.sequence.pad_sequences(token_answers,maxlen=m2,padding='post')\n",
        "coding=to_categorical(pad_answers,size)\n",
        "decoder_output_data=np.array(coding)\n",
        "print(decoder_output_data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564, 22) 22\n",
            "(564, 74) 74\n",
            "(564, 74, 1894)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vKiAKNg_rJW",
        "outputId": "5b0d8123-43f9-457b-b1a3-5057c4308ad8"
      },
      "source": [
        "encoder_inputs=Input(shape=(m,))\n",
        "encoder_embedding=Embedding(size,200,mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs,state_h,state_c=LSTM(200,return_state=True)(encoder_embedding)\n",
        "encoder_states=[state_h,state_c]\n",
        "\n",
        "decoder_inputs=Input(shape=(m2,))\n",
        "decoder_embedding=Embedding(size,200,mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm=LSTM(200,return_state=True,return_sequences=True)\n",
        "decoder_outputs,t1,t2=decoder_lstm(decoder_embedding,initial_state=encoder_states)\n",
        "decoder_dense=Dense(size,activation=tf.keras.activations.softmax) \n",
        "output=decoder_dense( decoder_outputs )\n",
        "\n",
        "model=Model([encoder_inputs,decoder_inputs],output)\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(),loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 22)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 74)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 22, 200)      378800      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 74, 200)      378800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 74, 200), (N 320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 74, 1894)     380694      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,779,894\n",
            "Trainable params: 1,779,894\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPnwBgS3A7D9",
        "outputId": "3ed0a641-2353-425d-e530-e3164c10f76e"
      },
      "source": [
        "model.fit([encoder_input_data,decoder_input_data],decoder_output_data,batch_size=50,epochs=150) \n",
        "model.save('temp.h5') "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 23s 35ms/step - loss: 1.4284\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 1.1906\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 1.1016\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 1.0321\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 1.0368\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 1.0293\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 1.0726\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 1.0399\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 1.0504\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.9937\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.9767\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.9628\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8935\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.9369\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8704\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.9582\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.8670\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8224\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.8540\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8399\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8400\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.8094\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.8316\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.7829\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.7742\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.7701\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.7406\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.7027\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.7804\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.7297\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.7148\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.6880\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6875\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6673\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.6464\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6869\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6570\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.6154\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6514\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.5787\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5829\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.5754\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.5619\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5736\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.5487\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.5552\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.5335\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5344\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.5596\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5176\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5204\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5047\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.5081\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4616\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.4631\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4936\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4784\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4398\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.4341\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4220\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4117\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3991\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.4059\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.3768\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.3789\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3911\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3782\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3602\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.3525\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3479\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3298\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3321\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3207\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.2964\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3156\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3155\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2964\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2911\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.3010\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.2853\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.2764\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2604\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.2641\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.2284\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.2371\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.2338\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2268\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2370\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.2294\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2194\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.2082\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2076\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.2179\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1969\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1964\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1813\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1840\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1816\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.1709\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1630\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1529\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1522\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.1613\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1562\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1461\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1515\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1420\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1399\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1352\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.1272\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1291\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1102\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1219\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.1118\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.1144\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.1114\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.1105\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0968\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.1027\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.1000\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0971\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0901\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0864\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.0865\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.0869\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0767\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0763\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0787\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0707\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.0704\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0716\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.0685\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0713\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.0685\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0696\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0622\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0547\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0624\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0536\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.0559\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0533\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0450\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.0511\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.0500\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.0418\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.0442\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 0.0435\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0428\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.0414\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.0406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcESCJrBpX3"
      },
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h=Input(shape=(200,))\n",
        "    decoder_state_input_c=Input(shape=(200,))\n",
        "    \n",
        "    decoder_states_inputs=[decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs,state_h,state_c=decoder_lstm(decoder_embedding,initial_state=decoder_states_inputs)\n",
        "    decoder_states=[state_h,state_c]\n",
        "    decoder_outputs=decoder_dense(decoder_outputs)\n",
        "    decoder_model=Model([decoder_inputs]+decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model,decoder_model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF7I-ASIByJ_"
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words=sentence.lower().split()\n",
        "    tokens_list=list()\n",
        "    for word in words:\n",
        "        tokens_list.append(tokenizer.word_index[word]) \n",
        "    return preprocessing.sequence.pad_sequences([tokens_list],maxlen=m,padding='post')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "N20gVmtkB4JD",
        "outputId": "74b6d37c-f700-4e06-8491-fe989dc82923"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values=enc_model.predict(str_to_tokens(input('Enter question : ')))\n",
        "    empty_target_seq=np.zeros((1,1))\n",
        "    empty_target_seq[0, 0]=tokenizer.word_index['start']\n",
        "    tt=False\n",
        "    final=''\n",
        "    while not tt :\n",
        "        dec_outputs,h,c=dec_model.predict([empty_target_seq]+states_values)\n",
        "        t_ind=np.argmax(dec_outputs[0,-1,:])\n",
        "        temp=None\n",
        "        for word,index in tokenizer.word_index.items() :\n",
        "            if t_ind == index :\n",
        "                final+=' {}'.format(word)\n",
        "                temp=word\n",
        "        \n",
        "        if temp=='end' or len(final.split()) > m:\n",
        "            tt=True\n",
        "            \n",
        "        empty_target_seq=np.zeros((1,1))  \n",
        "        empty_target_seq[0,0]=t_ind\n",
        "        states_values=[h,c] \n",
        "\n",
        "    print(final)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter question : hi\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 74) for input KerasTensor(type_spec=TensorSpec(shape=(None, 74), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n",
            " hello end\n",
            "Enter question : how are you\n",
            " i have no i'm a piece of software end\n",
            "Enter question : where are you\n",
            " i am on the internet end\n",
            "Enter question : why\n",
            " i support the 2nd amendment end\n",
            "Enter question : are you okay\n",
            " no i am sober nope not noticeably i'm software i'm not so i can be programmed to act and react as if i\n",
            "Enter question : do you talk\n",
            " no i am not into sports that myself end\n",
            "Enter question : do you like me\n",
            " what is basketball i frighten like that much myself end\n",
            "Enter question : quit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a13c9c91b2de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter question : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1bfc53165077>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtokens_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'quit'"
          ]
        }
      ]
    }
  ]
}